# -*- coding: utf-8 -*-
"""Chinese Tokenizer JIEBA

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DDhxNQ-BzRRZeYEhEa9AkIQds9942yti

# 结巴 from https://github.com/fxsjy/jieba
"""

import jieba
import jieba.posseg as pseg

text_zh="我来到北京清华大学"
text_tw="我是從韓國首爾來的韓文老師"

# Chinese simple
seg_list = jieba.cut(text_zh, cut_all=True)
print("Full Mode: " + "/ ".join(seg_list))  # 全模式

seg_list = jieba.cut(text_tw, cut_all=True)
print("Full Mode: " + "/ ".join(seg_list))  # 全模式

seg_list = jieba.cut("我是从韩国首尔来的韩文老师", cut_all=True)
print("Full Mode: " + "/ ".join(seg_list))  # 全模式

seg_list = jieba.cut(text_zh, cut_all=False)
print("Default Mode: " + "/ ".join(seg_list))  # 默认模式

seg_list = jieba.cut(text_tw, cut_all=False)
print("Default Mode: " + "/ ".join(seg_list))  # 默认模式

seg_list = jieba.cut("我是从韩国首尔来的韩文老师", cut_all=False)
print("Default Mode: " + "/ ".join(seg_list))  # 默认模式

words = pseg.cut(text_zh) #jieba默认模式
jieba.enable_paddle() #启动paddle模式。 0.40版之后开始支持，早期版本不支持
words = pseg.cut(text_zh,use_paddle=True) #paddle模式
for word, flag in words:
    print('%s %s' % (word, flag))

words = pseg.cut(text_tw,use_paddle=True) #paddle模式
for word, flag in words:
    print('%s %s' % (word, flag))

text="我是从韩国首尔来的韩文老师"
words = pseg.cut(text,use_paddle=True) #paddle模式
for word, flag in words:
    print('%s %s' % (word, flag))

text = "于吉大招叫什么。于吉怒气技叫什么"
words = pseg.cut(text)
for w in words:
    print('%s %s' % (w.word, w.flag))