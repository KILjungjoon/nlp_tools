{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgj0cvuNiEVrHgP0SsY+EC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KILjungjoon/nlp_tools/blob/main/Lemmatizer_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.What is a 'Lemmatization'?"
      ],
      "metadata": {
        "id": "hwCZo8YMjPLZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Lemma : a representative form of vocabulary in the dictionary.\n",
        "* Lemmatization : the process of converting a word to its base form, lemma\n",
        "> Caring → care,  stroke → strike,   boys → boy .\n",
        "* Also, sometimes, the same word can have multiple different ‘lemma’s. So, based on the context it’s used, you should identify the ‘part-of-speech’ (POS) tag for the word in that specific context and extract the appropriate lemma."
      ],
      "metadata": {
        "id": "A3Z0YofVXeg1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UnpHfOWoXKwp"
      },
      "outputs": [],
      "source": [
        "sentence = \"\"\"Our model has hitherto been to provide a basic social safety net through subsidised education, healthcare and housing, \n",
        "supplemented by social assistance programmes for the needy through a many-helping-hands approach.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Wordnet Lemmatizer with NLTK"
      ],
      "metadata": {
        "id": "siNFixonjKnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 몇몇 문제가 발견, 불완전.\n",
        "# NLTK\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "word_list = nltk.word_tokenize(sentence)\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
        "\n",
        "print(sentence)\n",
        "print(lemmatized_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiWiXYBHbU2z",
        "outputId": "2301584e-9b5f-429a-d42d-bb1073a57392"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our model has hitherto been to provide a basic social safety net through subsidised education, healthcare and housing, supplemented by social assistance programmes for the needy through a many-helping-hands approach.\n",
            "Our model ha hitherto been to provide a basic social safety net through subsidised education , healthcare and housing , supplemented by social assistance programme for the needy through a many-helping-hands approach .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Wordnet Lemmatizer with appropriate POS tag"
      ],
      "metadata": {
        "id": "wC8k0wMHjj1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 정확도가 많이 높아졌다! 오류 없음.\n",
        "# Lemmatize with POS Tag \n",
        "from nltk.corpus import wordnet\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "# 1. Init Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# 2. Lemmatize Single Word with the appropriate POS tag\n",
        "word = 'feet'\n",
        "print(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
        "\n",
        "# 3. Lemmatize a Sentence with the appropriate POS tag\n",
        "print(sentence)\n",
        "print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-0MN4XEj1Ae",
        "outputId": "66592d6c-7b62-4e5a-de8b-ea083d71e326"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "foot\n",
            "Our model has hitherto been to provide a basic social safety net through subsidised education, healthcare and housing, supplemented by social assistance programmes for the needy through a many-helping-hands approach.\n",
            "['Our', 'model', 'have', 'hitherto', 'be', 'to', 'provide', 'a', 'basic', 'social', 'safety', 'net', 'through', 'subsidise', 'education', ',', 'healthcare', 'and', 'housing', ',', 'supplement', 'by', 'social', 'assistance', 'programme', 'for', 'the', 'needy', 'through', 'a', 'many-helping-hands', 'approach', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.spaCy Lemmatization"
      ],
      "metadata": {
        "id": "hCS_6mAvlUpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 양호하다. subsidised만 오류\n",
        "# Spacy\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "doc = nlp(sentence)\n",
        "print(sentence)\n",
        "print(\" \".join([token.lemma_ for token in doc]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZeLj0xlcpAJ",
        "outputId": "f47797d3-335a-42dc-e2f9-9c597a49f30f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our model has hitherto been to provide a basic social safety net through subsidised education, healthcare and housing, supplemented by social assistance programmes for the needy through a many-helping-hands approach.\n",
            "our model have hitherto be to provide a basic social safety net through subsidised education , healthcare and housing , supplement by social assistance programme for the needy through a many - help - hand approach .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.TextBlob Lemmatizer"
      ],
      "metadata": {
        "id": "hIkvddL1lbLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MZpb5IulX3M",
        "outputId": "5caef430-f4f3-4e00-b093-8fc018eadb07"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# has, been, subsidised, supplemented 오류\n",
        "\n",
        "from textblob import TextBlob, Word\n",
        "\n",
        "# Lemmatize a word\n",
        "word = 'stripes'\n",
        "w = Word(word)\n",
        "print(w.lemmatize())\n",
        "\n",
        "# Lemmatize a sentence\n",
        "sent = TextBlob(sentence)\n",
        "print(sentence)\n",
        "print(\" \". join([w.lemmatize() for w in sent.words]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvPhbUjVlkHr",
        "outputId": "0b0c3464-6dff-4596-9f3a-273ad0962e96"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stripe\n",
            "Our model has hitherto been to provide a basic social safety net through subsidised education, healthcare and housing, supplemented by social assistance programmes for the needy through a many-helping-hands approach.\n",
            "Our model ha hitherto been to provide a basic social safety net through subsidised education healthcare and housing supplemented by social assistance programme for the needy through a many-helping-hands approach\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.TextBlob Lemmatizer with appropriate POS tag"
      ],
      "metadata": {
        "id": "pSB4cWkCmUUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GOOD! 오류 없음.\n",
        "# Define function to lemmatize each word with its POS tag\n",
        "def lemmatize_with_postag(sentence):\n",
        "    sent = TextBlob(sentence)\n",
        "    tag_dict = {\"J\": 'a', \n",
        "                \"N\": 'n', \n",
        "                \"V\": 'v', \n",
        "                \"R\": 'r'}\n",
        "    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]    \n",
        "    lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n",
        "    return \" \".join(lemmatized_list)\n",
        "\n",
        "# Lemmatize\n",
        "print(sentence)\n",
        "print(lemmatize_with_postag(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j7JvSgnmXI6",
        "outputId": "6115af58-39c4-424f-98c1-37b5c8d8d142"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our model has hitherto been to provide a basic social safety net through subsidised education, healthcare and housing, supplemented by social assistance programmes for the needy through a many-helping-hands approach.\n",
            "Our model have hitherto be to provide a basic social safety net through subsidise education healthcare and housing supplement by social assistance programme for the needy through a many-helping-hands approach\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.Pattern Lemmatizer"
      ],
      "metadata": {
        "id": "FUwmU6Q0nQNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pattern"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Q2OOOqQnQxy",
        "outputId": "2b4e193e-e12a-4ba9-b369-afba53082e57"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pattern\n",
            "  Downloading Pattern-3.6.0.tar.gz (22.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.2 MB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pattern) (0.16.0)\n",
            "Collecting backports.csv\n",
            "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
            "Collecting mysqlclient\n",
            "  Downloading mysqlclient-2.1.1.tar.gz (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from pattern) (4.6.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from pattern) (4.9.1)\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 257 kB/s \n",
            "\u001b[?25hCollecting pdfminer.six\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 20.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pattern) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pattern) (1.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pattern) (3.7)\n",
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 44.9 MB/s \n",
            "\u001b[?25hCollecting cherrypy\n",
            "  Downloading CherryPy-18.8.0-py2.py3-none-any.whl (348 kB)\n",
            "\u001b[K     |████████████████████████████████| 348 kB 50.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pattern) (2.23.0)\n",
            "Collecting portend>=2.1.1\n",
            "  Downloading portend-3.1.0-py3-none-any.whl (5.3 kB)\n",
            "Collecting zc.lockfile\n",
            "  Downloading zc.lockfile-2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from cherrypy->pattern) (9.0.0)\n",
            "Collecting jaraco.collections\n",
            "  Downloading jaraco.collections-3.8.0-py3-none-any.whl (10 kB)\n",
            "Collecting cheroot>=8.2.1\n",
            "  Downloading cheroot-9.0.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[K     |████████████████████████████████| 100 kB 8.3 MB/s \n",
            "\u001b[?25hCollecting jaraco.functools\n",
            "  Downloading jaraco.functools-3.5.2-py3-none-any.whl (7.3 kB)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from cheroot>=8.2.1->cherrypy->pattern) (1.15.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from cheroot>=8.2.1->cherrypy->pattern) (4.13.0)\n",
            "Collecting tempora>=1.8\n",
            "  Downloading tempora-5.1.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2022.6)\n",
            "Collecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->cheroot>=8.2.1->cherrypy->pattern) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->cheroot>=8.2.1->cherrypy->pattern) (3.10.0)\n",
            "Collecting jaraco.text\n",
            "  Downloading jaraco.text-3.11.0-py3-none-any.whl (11 kB)\n",
            "Collecting jaraco.classes\n",
            "  Downloading jaraco.classes-3.2.3-py3-none-any.whl (6.0 kB)\n",
            "Collecting jaraco.context>=4.1\n",
            "  Downloading jaraco.context-4.2.0-py3-none-any.whl (5.0 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (5.10.0)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (2.1.0)\n",
            "Collecting autocommand\n",
            "  Downloading autocommand-2.2.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->pattern) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->pattern) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->pattern) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk->pattern) (4.64.1)\n",
            "Collecting cryptography>=36.0.0\n",
            "  Downloading cryptography-38.0.3-cp36-abi3-manylinux_2_24_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 35.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->pattern) (2.1.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=36.0.0->pdfminer.six->pattern) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pattern) (2.21)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (1.24.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zc.lockfile->cherrypy->pattern) (57.4.0)\n",
            "Building wheels for collected packages: pattern, mysqlclient, python-docx, sgmllib3k\n",
            "  Building wheel for pattern (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pattern: filename=Pattern-3.6-py3-none-any.whl size=22332722 sha256=d559a6bf23351bf5089af360a992dd5bade50f45d9cd68ebf805d7e5d169c7b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/1f/4e/9b67afd2430d55dee90bd57618dd7d899f1323e5852c465682\n",
            "  Building wheel for mysqlclient (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mysqlclient: filename=mysqlclient-2.1.1-cp37-cp37m-linux_x86_64.whl size=99987 sha256=3f59f4878b8e7afb56d5da028c7127b8ed4ec327a988e8c3aa5d32f4ee97547d\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/2d/67/2cb3f82e435fc8e055cb2761a15a0812bf086068f6fb835462\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184505 sha256=c84482adfec82571f8175cb19030ba17f78c088c37186608f2ae5bb59cc90ddb\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=a0a80b263e75d6e8438593a636ba410c7a155ead790f89c1aae4eed8f893c61c\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
            "Successfully built pattern mysqlclient python-docx sgmllib3k\n",
            "Installing collected packages: jaraco.functools, jaraco.context, autocommand, tempora, jaraco.text, jaraco.classes, zc.lockfile, sgmllib3k, portend, jaraco.collections, cryptography, cheroot, python-docx, pdfminer.six, mysqlclient, feedparser, cherrypy, backports.csv, pattern\n",
            "Successfully installed autocommand-2.2.2 backports.csv-1.0.7 cheroot-9.0.0 cherrypy-18.8.0 cryptography-38.0.3 feedparser-6.0.10 jaraco.classes-3.2.3 jaraco.collections-3.8.0 jaraco.context-4.2.0 jaraco.functools-3.5.2 jaraco.text-3.11.0 mysqlclient-2.1.1 pattern-3.6 pdfminer.six-20221105 portend-3.1.0 python-docx-0.8.11 sgmllib3k-1.0.0 tempora-5.1.0 zc.lockfile-2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GOOD! 오류 없음.\n",
        "\n",
        "from nltk.corpus.reader.semcor import SemcorSentence\n",
        "import pattern\n",
        "from pattern.en import lemma, lexeme\n",
        "\n",
        "\n",
        "print(sentence)\n",
        "print(\" \".join([lemma(wd) for wd in sentence.split()]))\n",
        "\n",
        "# Lexeme's for each word \n",
        "print([lexeme(wd) for wd in sentence.split()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LI7VZVJRnfOp",
        "outputId": "de23d331-abad-44c6-f255-c4d03c9b2905"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our model has hitherto been to provide a basic social safety net through subsidised education, healthcare and housing, supplemented by social assistance programmes for the needy through a many-helping-hands approach.\n",
            "our model have hitherto be to provide a basic social safety net through subsidise education, healthcare and housing, supplement by social assistance programme for the needy through a many-helping-hand approach.\n",
            "[['our', 'ours', 'ouring', 'oured'], ['model', 'models', 'modelling', 'modelled'], ['have', 'has', 'having', 'had', \"haven't\", \"hasn't\", \"hadn't\"], ['hitherto', 'hithertos', 'hithertoing', 'hithertoed'], ['be', 'am', 'are', 'is', 'being', 'was', 'were', 'been', 'am not', \"aren't\", \"isn't\", \"wasn't\", \"weren't\"], ['to', 'tos', 'toing', 'toed'], ['provide', 'provides', 'providing', 'provided'], ['a', 'as', 'aing', 'aed'], ['basic', 'basices', 'basicking', 'basicked'], ['social', 'socials', 'socialing', 'socialed'], ['safety', 'safeties', 'safetying', 'safetied'], ['net', 'nets', 'netting', 'netted'], ['through', 'throughs', 'throughing', 'throughed'], ['subsidise', 'subsidises', 'subsidising', 'subsidised'], ['education,', 'education,s', 'education,ing', 'education,ed'], ['healthcare', 'healthcares', 'healthcaring', 'healthcared'], ['and', 'ands', 'anding', 'anded'], ['housing,', 'housing,s', 'housing,ing', 'housing,ed'], ['supplement', 'supplements', 'supplementing', 'supplemented'], ['by', 'bies', 'bying', 'bied'], ['social', 'socials', 'socialing', 'socialed'], ['assistance', 'assistances', 'assistancing', 'assistanced'], ['programme', 'programs', 'programming', 'programmed'], ['for', 'fors', 'forring', 'forred'], ['the', 'thes', 'thing', 'thed'], ['needy', 'needies', 'needying', 'needied'], ['through', 'throughs', 'throughing', 'throughed'], ['a', 'as', 'aing', 'aed'], ['many-helping-hand', 'many-helping-hands', 'many-helping-handing', 'many-helping-handed'], ['approach.', 'approach.s', 'approach.ing', 'approach.ed']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain the lemma by parsing the sentence : sentence를 직접 스트링으로 넣어줘야 한다.\n",
        "from pattern.en import parse\n",
        "print(parse(\"\"\"Our model has hitherto been to provide a basic social safety net through subsidised education, healthcare and housing, \n",
        "supplemented by social assistance programmes for the needy through a many-helping-hands approach.\"\"\", lemmata=True, tags=False, chunks=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjoA4yHvpLWf",
        "outputId": "52acbc78-dc05-41a5-9e8f-931e9acee74b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our/PRP$/our model/NN/model has/VBZ/have hitherto/RB/hitherto been/VBN/be to/TO/to provide/VB/provide a/DT/a basic/JJ/basic social/JJ/social safety/NN/safety net/NN/net through/IN/through subsidised/JJ/subsidised education/NN/education ,/,/, healthcare/NN/healthcare and/CC/and housing/NN/housing ,/,/, supplemented/VBN/supplement by/IN/by social/JJ/social assistance/NN/assistance programmes/NNS/programme for/IN/for the/DT/the needy/JJ/needy through/IN/through a/DT/a many-helping-hands/NNS/many-helping-hand approach/NN/approach ././.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.Stanford CoreNLP Lemmatization\n",
        "* 서버 접속 문제로 pass"
      ],
      "metadata": {
        "id": "p8cJiDYzpwmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanfordcorenlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPFTwB10qGXU",
        "outputId": "7ee3b058-ec80-4f7b-cd75-d06a465c75d8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: stanfordcorenlp in /usr/local/lib/python3.7/dist-packages (3.9.1.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from stanfordcorenlp) (5.4.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stanfordcorenlp) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordcorenlp) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordcorenlp) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordcorenlp) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordcorenlp) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stanfordcorenlp import StanfordCoreNLP\n",
        "import json\n",
        "\n",
        "# Connect to the CoreNLP server we just started\n",
        "nlp = StanfordCoreNLP('http://localhost', port=9000, timeout=30000)\n",
        "\n",
        "# Define proporties needed to get lemma\n",
        "props = {'annotators': 'pos,lemma',\n",
        "         'pipelineLanguage': 'en',\n",
        "         'outputFormat': 'json'}\n",
        "\n",
        "parsed_str = nlp.annotate(sentence, properties=props)\n",
        "parsed_dict = json.loads(parsed_str)\n",
        "parsed_dict"
      ],
      "metadata": {
        "id": "Q0eHptcNqg80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9.Gensim Lemmatize\n",
        "* generator raised StopIteration 오류로 pass"
      ],
      "metadata": {
        "id": "rZ4Kcjc-0Hkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.utils import lemmatize\n",
        "print(sentence)\n",
        "lemmatized_out = [wd.decode('utf-8').split('/')[0] for wd in lemmatize(sentence)]"
      ],
      "metadata": {
        "id": "mhR0fg7B0NI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10.TreeTagger\n",
        "* 다운로드 등 설정이 번거롭다. pass"
      ],
      "metadata": {
        "id": "bxva1DGi3f09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install treetaggerwrapper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVxs2s6L3ovR",
        "outputId": "d3bb3e90-85e9-41f7-d11d-b1548cff844c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting treetaggerwrapper\n",
            "  Downloading treetaggerwrapper-2.3.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: treetaggerwrapper\n",
            "  Building wheel for treetaggerwrapper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for treetaggerwrapper: filename=treetaggerwrapper-2.3-py3-none-any.whl size=40773 sha256=d95281a89e34de115809fb1cd71c6fa5c45229185dee43a2d46342b97869985c\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/93/50/47079639c52033b2e2b865a59654eea6832068149414cb78a5\n",
            "Successfully built treetaggerwrapper\n",
            "Installing collected packages: treetaggerwrapper\n",
            "Successfully installed treetaggerwrapper-2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import treetaggerwrapper as ttpw\n",
        "tagger = ttpw.TreeTagger(TAGLANG='en', TAGDIR='/Users/ecom-selva.p/Documents/MLPlus/11_Lemmatization/treetagger')\n",
        "tags = tagger.tag_text(sentence)\n",
        "print(sentence)\n",
        "print(lemmas = [t.split('\\t')[-1] for t in tags])"
      ],
      "metadata": {
        "id": "46uGB1LN37AL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "< 결론 > 오류가 없었던 3가지 방법은 다음과 같다.\n",
        "* 3.Wordnet Lemmatizer with appropriate POS tag\n",
        "* 6.TextBlob Lemmatizer with appropriate POS tag\n",
        "* 7.Pattern Lemmatizer\n",
        "---\n",
        "< Reference >\n",
        "* https://www.machinelearningplus.com/nlp/lemmatization-examples-python/"
      ],
      "metadata": {
        "id": "xY8n3nHO2sVq"
      }
    }
  ]
}